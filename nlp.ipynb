{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gutenberg', 'gutenberg.zip', 'movie_reviews', 'movie_reviews.zip', 'stopwords', 'stopwords.zip', 'wordnet', 'wordnet.zip', 'words', 'words.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Name\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
     ]
    }
   ],
   "source": [
    "for i in hamlet[:500]:\n",
    "    print(i,end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI=' Artificial intelligence is a science and technology based on disciplines such as Computer Science, Biology, Psychology,\\\n",
    "      Linguistics, Mathematics, and Engineering. A major thrust of AI is in the development of computer functions associated with human intelligence, \\\n",
    "      such as reasoning, learning, and problem solving'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Name\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " 'is',\n",
       " 'a',\n",
       " 'science',\n",
       " 'and',\n",
       " 'technology',\n",
       " 'based',\n",
       " 'on',\n",
       " 'disciplines',\n",
       " 'such',\n",
       " 'as',\n",
       " 'Computer',\n",
       " 'Science',\n",
       " ',',\n",
       " 'Biology',\n",
       " ',',\n",
       " 'Psychology',\n",
       " ',',\n",
       " 'Linguistics',\n",
       " ',',\n",
       " 'Mathematics',\n",
       " ',',\n",
       " 'and',\n",
       " 'Engineering',\n",
       " '.',\n",
       " 'A',\n",
       " 'major',\n",
       " 'thrust',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'development',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'functions',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'reasoning',\n",
       " ',',\n",
       " 'learning',\n",
       " ',',\n",
       " 'and',\n",
       " 'problem',\n",
       " 'solving']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_tokens = word_tokenize(AI)\n",
    "ai_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ai_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist =FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 8,\n",
       "          '.': 1,\n",
       "          'a': 2,\n",
       "          'ai': 1,\n",
       "          'and': 3,\n",
       "          'artificial': 1,\n",
       "          'as': 2,\n",
       "          'associated': 1,\n",
       "          'based': 1,\n",
       "          'biology': 1,\n",
       "          'computer': 2,\n",
       "          'development': 1,\n",
       "          'disciplines': 1,\n",
       "          'engineering': 1,\n",
       "          'functions': 1,\n",
       "          'human': 1,\n",
       "          'in': 1,\n",
       "          'intelligence': 2,\n",
       "          'is': 2,\n",
       "          'learning': 1,\n",
       "          'linguistics': 1,\n",
       "          'major': 1,\n",
       "          'mathematics': 1,\n",
       "          'of': 2,\n",
       "          'on': 1,\n",
       "          'problem': 1,\n",
       "          'psychology': 1,\n",
       "          'reasoning': 1,\n",
       "          'science': 2,\n",
       "          'solving': 1,\n",
       "          'such': 2,\n",
       "          'technology': 1,\n",
       "          'the': 1,\n",
       "          'thrust': 1,\n",
       "          'with': 1})"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in ai_tokens:\n",
    "    fdist[word.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.probability.FreqDist"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 8),\n",
       " ('and', 3),\n",
       " ('intelligence', 2),\n",
       " ('is', 2),\n",
       " ('a', 2),\n",
       " ('science', 2),\n",
       " ('such', 2),\n",
       " ('as', 2),\n",
       " ('computer', 2),\n",
       " ('of', 2)]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10 = fdist.most_common(10)\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "ai_blank = blankline_tokenize(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ai_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams,trigrams,ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'best',\n",
       " 'and',\n",
       " 'beautiful',\n",
       " 'things',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'seen',\n",
       " 'or',\n",
       " 'even',\n",
       " 'touched',\n",
       " 'they',\n",
       " 'must',\n",
       " 'be',\n",
       " 'felt',\n",
       " 'with',\n",
       " 'the',\n",
       " 'heart']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = ' the best and beautiful things in the world cannot be seen or even touched they must be felt with the heart'\n",
    "quotes_tokens = nltk.word_tokenize(string)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'best'),\n",
       " ('best', 'and'),\n",
       " ('and', 'beautiful'),\n",
       " ('beautiful', 'things'),\n",
       " ('things', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'world'),\n",
       " ('world', 'can'),\n",
       " ('can', 'not'),\n",
       " ('not', 'be'),\n",
       " ('be', 'seen'),\n",
       " ('seen', 'or'),\n",
       " ('or', 'even'),\n",
       " ('even', 'touched'),\n",
       " ('touched', 'they'),\n",
       " ('they', 'must'),\n",
       " ('must', 'be'),\n",
       " ('be', 'felt'),\n",
       " ('felt', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'heart')]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "quote_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'best', 'and', 'beautiful'),\n",
       " ('best', 'and', 'beautiful', 'things'),\n",
       " ('and', 'beautiful', 'things', 'in'),\n",
       " ('beautiful', 'things', 'in', 'the'),\n",
       " ('things', 'in', 'the', 'world'),\n",
       " ('in', 'the', 'world', 'can'),\n",
       " ('the', 'world', 'can', 'not'),\n",
       " ('world', 'can', 'not', 'be'),\n",
       " ('can', 'not', 'be', 'seen'),\n",
       " ('not', 'be', 'seen', 'or'),\n",
       " ('be', 'seen', 'or', 'even'),\n",
       " ('seen', 'or', 'even', 'touched'),\n",
       " ('or', 'even', 'touched', 'they'),\n",
       " ('even', 'touched', 'they', 'must'),\n",
       " ('touched', 'they', 'must', 'be'),\n",
       " ('they', 'must', 'be', 'felt'),\n",
       " ('must', 'be', 'felt', 'with'),\n",
       " ('be', 'felt', 'with', 'the'),\n",
       " ('felt', 'with', 'the', 'heart')]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_ngrams = list(nltk.ngrams(quotes_tokens,4))\n",
    "quote_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('gives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : give\n",
      "given : given\n",
      "gave : gave\n"
     ]
    }
   ],
   "source": [
    "word_to_stem=['give','giving','given','gave']\n",
    "for words in word_to_stem:\n",
    "    print(words,':',pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abnorm'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst.stem('abnormal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : giv\n",
      "giving : giv\n",
      "given : giv\n",
      "gave : gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "for word in word_to_stem:\n",
    "    print(word,':',lst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_len =WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Name\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : giving\n",
      "given : given\n",
      "gave : gave\n"
     ]
    }
   ],
   "source": [
    "for words in word_to_stem:\n",
    "    print(words,':',word_len.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Name\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 8,\n",
       "          '.': 1,\n",
       "          'a': 2,\n",
       "          'ai': 1,\n",
       "          'and': 3,\n",
       "          'artificial': 1,\n",
       "          'as': 2,\n",
       "          'associated': 1,\n",
       "          'based': 1,\n",
       "          'biology': 1,\n",
       "          'computer': 2,\n",
       "          'development': 1,\n",
       "          'disciplines': 1,\n",
       "          'engineering': 1,\n",
       "          'functions': 1,\n",
       "          'human': 1,\n",
       "          'in': 1,\n",
       "          'intelligence': 2,\n",
       "          'is': 2,\n",
       "          'learning': 1,\n",
       "          'linguistics': 1,\n",
       "          'major': 1,\n",
       "          'mathematics': 1,\n",
       "          'of': 2,\n",
       "          'on': 1,\n",
       "          'problem': 1,\n",
       "          'psychology': 1,\n",
       "          'reasoning': 1,\n",
       "          'science': 2,\n",
       "          'solving': 1,\n",
       "          'such': 2,\n",
       "          'technology': 1,\n",
       "          'the': 1,\n",
       "          'thrust': 1,\n",
       "          'with': 1})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation = re.compile(r'[-.?!,:;()|0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_punctuation=[]\n",
    "for words in ai_tokens:\n",
    "    word = punctuation.sub(\"\",words)\n",
    "    if (len(word))>0:\n",
    "        post_punctuation.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " 'is',\n",
       " 'a',\n",
       " 'science',\n",
       " 'and',\n",
       " 'technology',\n",
       " 'based',\n",
       " 'on',\n",
       " 'disciplines',\n",
       " 'such',\n",
       " 'as',\n",
       " 'Computer',\n",
       " 'Science',\n",
       " 'Biology',\n",
       " 'Psychology',\n",
       " 'Linguistics',\n",
       " 'Mathematics',\n",
       " 'and',\n",
       " 'Engineering',\n",
       " 'A',\n",
       " 'major',\n",
       " 'thrust',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'development',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'functions',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " 'such',\n",
       " 'as',\n",
       " 'reasoning',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'problem',\n",
       " 'solving']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'timothy is a natural when it comes to drawing'\n",
    "sent_tokens = word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Name\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('timothy', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('natural', 'JJ')]\n",
      "[('when', 'WRB')]\n",
      "[('it', 'PRP')]\n",
      "[('comes', 'VBZ')]\n",
      "[('to', 'TO')]\n",
      "[('drawing', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "for token in sent_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_sent = 'The US President stays in the WHITE HOUSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tokens = word_tokenize(NE_sent)\n",
    "ne_tags = nltk.pos_tag(ne_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Name\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Name\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entiry Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION US/NNP)\n",
      "  President/NNP\n",
      "  stays/VBZ\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (FACILITY WHITE/NNP HOUSE/NNP))\n"
     ]
    }
   ],
   "source": [
    "ne_ner = ne_chunk(ne_tags)\n",
    "print(ne_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('cat', 'NN'),\n",
       " ('ate', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('mouse', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('was', 'VBD'),\n",
       " ('after', 'IN'),\n",
       " ('fresh', 'JJ'),\n",
       " ('cheese', 'NN')]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = \"the big cat ate the little mouse who was after fresh cheese\"\n",
    "new_tokens = nltk.pos_tag(word_tokenize(new))\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer_np = r'NP: {<DT>?<JJ>*<NN>}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser = nltk.RegexpParser(grammer_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT big/JJ cat/NN)\n",
      "  ate/VBD\n",
      "  (NP the/DT little/JJ mouse/NN)\n",
      "  who/WP\n",
      "  was/VBD\n",
      "  after/IN\n",
      "  (NP fresh/JJ cheese/NN))\n"
     ]
    }
   ],
   "source": [
    "chunk_results = chunk_parser.parse(new_tokens)\n",
    "print(chunk_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gutenberg', 'gutenberg.zip', 'movie_reviews', 'movie_reviews.zip', 'stopwords', 'stopwords.zip', 'wordnet', 'wordnet.zip', 'words', 'words.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find('corpora')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Name\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rev=movie_reviews.fileids('pos')\n",
    "neg_rev=movie_reviews.fileids('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev = movie_reviews.words('pos/cv000_29590.txt')\n",
    "rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_list=[]\n",
    "for rev in neg_rev:\n",
    "    rev_text_neg = nltk.corpus.movie_reviews.words(rev)\n",
    "    review_one_string = \"\".join(rev_text_neg)\n",
    "    review_one_string = review_one_string.replace(' ,',',')\n",
    "    review_one_string = review_one_string.replace(' .','.')\n",
    "    review_one_string = review_one_string.replace(\"\\' \",\"'\")\n",
    "    review_one_string = review_one_string.replace(\" \\'\",\"'\")\n",
    "    rev_list.append(review_one_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rev in pos_rev:\n",
    "    rev_text_pos = nltk.corpus.movie_reviews.words(rev)\n",
    "    review_one_string = \"\".join(rev_text_pos)\n",
    "    review_one_string = review_one_string.replace(' ,',',')\n",
    "    review_one_string = review_one_string.replace(' .','.')\n",
    "    review_one_string = review_one_string.replace(\"\\' \",\"'\")\n",
    "    review_one_string = review_one_string.replace(\" \\'\",\"'\")\n",
    "    rev_list.append(review_one_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_targets = np.zeros((1000,),dtype=np.int)\n",
    "pos_targets = np.ones((1000,),dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list=[]\n",
    "for neg_tar in neg_targets:\n",
    "    target_list.append(neg_tar)\n",
    "for pos_tar in pos_targets:\n",
    "    target_list.append(pos_tar)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target_list)\n",
    "y= pd.Series(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(lowercase=True,stop_words='english',min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_count_vect = count_vect.fit_transform(rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 9254)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9254"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_names = count_vect.get_feature_names()\n",
    "len(x_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_count_vect = pd.DataFrame(x_count_vect.toarray(),columns=x_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 9254)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000leaguesunderthesea</th>\n",
       "      <th>000years</th>\n",
       "      <th>05p</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>100million</th>\n",
       "      <th>100millionmark</th>\n",
       "      <th>101dalmatians</th>\n",
       "      <th>...</th>\n",
       "      <th>zack</th>\n",
       "      <th>zakorth</th>\n",
       "      <th>zane</th>\n",
       "      <th>zeik</th>\n",
       "      <th>zellweger</th>\n",
       "      <th>zero</th>\n",
       "      <th>zerobarrier</th>\n",
       "      <th>zeta</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9254 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  000leaguesunderthesea  000years  05p  10  100  100million  \\\n",
       "0   0    0                      0         0    0  10    0           0   \n",
       "1   0    0                      0         0    0   0    0           0   \n",
       "2   0    0                      0         0    0   0    0           0   \n",
       "3   0    0                      0         0    0   0    0           0   \n",
       "4   0    0                      0         0    0   0    0           0   \n",
       "\n",
       "   100millionmark  101dalmatians   ...    zack  zakorth  zane  zeik  \\\n",
       "0               0              0   ...       0        0     0     0   \n",
       "1               0              0   ...       0        0     0     0   \n",
       "2               0              0   ...       0        0     0     0   \n",
       "3               0              0   ...       0        0     0     0   \n",
       "4               0              0   ...       0        0     0     0   \n",
       "\n",
       "   zellweger  zero  zerobarrier  zeta  zip  zombie  \n",
       "0          0     0            0     0    0       0  \n",
       "1          0     0            0     0    0       0  \n",
       "2          0     0            0     0    0       0  \n",
       "3          0     0            0     0    0       0  \n",
       "4          0     0            0     0    0       0  \n",
       "\n",
       "[5 rows x 9254 columns]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count_vect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_count_vect,y,test_size=0.25,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 9254)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 9254)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred_gnb = gnb.fit(x_train,y_train).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_cv=MultinomialNB()\n",
    "clf_cv.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_cv = clf_cv.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.686\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,ypred_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[180,  78],\n",
       "       [ 79, 163]], dtype=int64)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_clf_cv = confusion_matrix(y_test,ypred_cv)\n",
    "score_clf_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n",
      "{'a': 1, 'c': 2}\n"
     ]
    }
   ],
   "source": [
    "li=[['a',1],['b',2],['c',2]]\n",
    "empt =dict(li)\n",
    "li=[]\n",
    "high=max(empt.values())\n",
    "for i in empt:\n",
    "    if empt[i]==high:\n",
    "        li.append(i)\n",
    "high1=li[0]\n",
    "for i in li:\n",
    "    if ord(i)<ord(high1):\n",
    "        high1=ord(i)\n",
    "print(high1)\n",
    "del empt[high1]\n",
    "print(empt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(a=1,b=2,c=2):\n",
    "    print(\"%f%d%d\"%(a,b,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00000022\n"
     ]
    }
   ],
   "source": [
    "g(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [5,2,1,65,23]\n",
    "for i in range(len(l1)):\n",
    "    for j in range(len(l1)-i-1):\n",
    "        if l1[j]>l1[j+1]:\n",
    "            l1[j],l1[j+1] = l1[j+1],l1[j]u"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
